<!doctype html>
<html>

<head>
  <title>MLLM-as-a-Judge Benchmark</title>
  <link rel="icon" href="img/mllm-logo.png" type="image/icon type">
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="/dist/output.css" rel="stylesheet">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <script src="js/index.js"></script>

  <style>

  </style>
</head>





<body>

  <div class="menu-container"></div>
<div class="fullscreen-background">
  <img src="img/website-background_00.png"> <!-- 更改为你的图片路径 -->
  <div class="gradient-text"> MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</div>
</div>
  <div id="body">
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">

        <h1 class="supportTitle">Introduction</h1>
        <br>
        <hr>
        <div class="arch">
          <img src="img//mllm-pipeline.png" class="title-icon2" alt="Rank Card">
        </div>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
              In this work, we introduce MLLM-as-a-Judge which thoroughly explores three types of Multimodal LLM-as-a-Judge in Vision-Language settings. Specifically, there are three-fold major contributions:
              <ol>
                <li><b>A Benchmark.</b> We are the first to develop a comprehensive benchmark MLLM-AS-A-JUDGE in multimodal domains, with human annotations to assess the judging capability of MLLMs in tasks of Scoring Evaluation, Pair Comparison and Batch Ranking. </li>
                <li><b>Two Datasets.</b> We curate two human preference datasets with high-quality questions MLLM-AS-A-JUDGE-HQ and MLLM-AS-A-JUDGE-HARD dataset with hallucination instances. They can serve as a rigorous testing ground to facilitate the development of MLLMs. </li>
                <li><b>Findings and Implications.</b> Our evaluation of mainstream MLLMs reveals that while MLLMs exhibit alignment with human judgments in pair comparison tasks, notable discrepancies can be found in scoring evaluation and batch ranking. Furthermore, our findings reveal that MLLMs exhibit a range of biases and hallucinations, along with inconsistent judgments during the evaluation process, representing significant hurdles in establishing MLLMs as reliable judges.</li>
              </ol>
            </p>
          </div>
        </div>
        <h1 class="supportTitle">Takeaway</h1>
        <br>
        <hr>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
              While the MLLM (e.g., LLaVA and GPT-4V) demonstrates superior performance in certain datasets and inferior performance in others, 
              we wish to underscore that our MLLM-as-a-Judge method leads to <b>a solid conclusion</b>: GPT-4V consistently outperforms the baseline 
              across diverse datasets on average. However, it remains noteworthy that GPT-4V does not entirely supplant human judges in particular 
              datasets, as elaborated in Section 4.1 of our paper. This overarching perspective on benchmarking MLLM-as-a-Judge underscores the 
              central focus of our study, which aims to assess MLLM performance from a comprehensive standpoint rather than evaluating individual 
              MLLM performance on specific datasets.
            </p>
          </div>
        </div>

        <h1 class="supportTitle">Empirical Findings</h1>
        <br>

        <h2 class="supportTitle2">Overall Assessments</h2>

        <div class="featurecard-container">
          <!-- Trustworthiness and Utility -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>MLLM Judgment vs Human Annotation</h2>
            </div>
            <div class="description">
              <p><ol>
                <li><b>Scoring Evaluation: </b>GPT-4V demonstrated the highest similarity to human scoring with a similarity score of 0.557. 
                  In contrast, Gemini achieved only 0.332, with LLaVA and CogVLM scoring even lower.
                  This discrepancy is primarily due to Gemini’s tendency to assign scores around 4 points, seldom giving 1 or 2 points. 
                  LLaVA and CogVLM show a similar pattern to Gemini, predominantly assigning scores around 4 points. 
                  We attribute this to a ‘High-Score’ Bias, akin to the ‘Yes/No’ bias, 
                  which may result from an imbalance in positive and negative judging instructions in their training data, 
                  severely limits their ability to provide just and varied scores in scoring settings. 
                  In comparison, GPT-4V’s scores are more evenly distributed and align closely with human preferences.</li>
                <li><b>Pair Comparison: </b>GPT-4V outshines other MLLMs in pair comparison tasks, achieving 0.683 in tie settings and 0.806 in non-tie settings, 
                  surpassing 0.8 in many datasets, which indicate a strong alignment with human preferences. 
                  Gemini, LLaVA, and CogVLM show a marked preference for declaring a clear winner, possibly due to a lack of tie situations in their training, leading to biased judgments. 
                  It’s also interesting that the frequency of ties given by GPT-4V closely mirrors that of human judges, suggesting similar thresholds for tie decisions.</li>
                <li><b>Batch Ranking: </b> GPT-4V aligns more closely with human ranking results, indicating a significant lead with a mean Levenshtein Distance of 0.313. 
                  However, there is still substantial room for improvement in this task for all MLLMs. Notably, CogVLM is unable to provide a full ranking in this context, 
                  offering only the top choice; so it was excluded from this comparison; LLaVA also exhibits position bias influenced by prompt structure, 
                  often replicating judgments seen in example prompts, which complicates its ability to produce fair judgments.</li>
              </ol></p>
            </div>
          </div>

          <!-- Alignment of LLMs -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>MLLM Judging Consistency</h2>
            </div>
            <div class="description">
              <p>
                To be a reliable judge, consistent decision-making across repeated evaluations 
                of the same query is crucial. For this purpose, we conducted six repeated tests
                with MLLM judgments and calculated the weighted average consistency scores and 
                Majority Consistency Criterion ratios for GPT-4V and Gemini. 
                Despite a higher temperature setting, GPT-4V substantially outperforms Gemini across all tasks. 
                Particularly in Pair Comparison, GPT-4V achieves a higher consistency score of 0.675, 
                but it encounters difficulties in maintaining similar levels of consistency in Scoring and Batch Ranking tasks, 
                with scores dropping to 0.611 and 0.418, indicating the challenge of producing qualified and convincing judgments.
              </p>
            </div>
          </div>

          <!-- Performance Gap in Trustworthiness -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>Vision Perception benefits Judging</h2>
            </div>
            <div class="description">
              <p>
                We explore the feasibility of using LLMs for judging textbased responses without directly analyzing the original images. 
                This involves two approaches: omitting vision information entirely and providing a detailed description of the picture. 
                Surprisingly, we find that LLMs’ performance in multimodal judging tasks significantly improved with picture descriptions, 
                achieving a Pearson similarity of 0.435 in Scoring Evaluation tasks, markedly outperformed judgments made without any vision
                perception. Notably, in non-tie Pair Comparison, MLLMs with detailed vision descriptions even exceed the standard performance of MLLMs in judging. 
                This suggests that MLLMs may lack certain human-like judging capabilities, while LLMs can effectively judge multimodal tasks when provided with comprehensive task-related descriptions.
              </p>
            </div>
          </div>


          <!-- Transparency in Trustworthiness -->
          <div class="feature_1x1">

            <div class="featurecard">
              <h2>Human Agreement</h2>
            </div>
            <div class="description">
              <p>Our manual evaluation of MLLMs in judging, focusing on agreement and scoring, revealed notable findings. 
                GPT-4V achieved around 70% human agreement across all settings, excelling in the Pair Comparison task with 79.3% agreement. 
                Specifically, GPT-4V reached 78% in human agreement for Pair Comparison, with Gemini close at 72%, indicating strong performance 
                in most sample pairs and supporting the idea that large models excel in pairwise distinctions (Zheng et al., 2023b), though improvements 
                are needed in other judging settings. 
                
                In the Scoring Evaluation task, GPT-4V achieved a 70% human agreement rate, peaking at 79.9% in MS-COCO, 
                while Gemini maintained an average rate of 67.7%. To assess the consistency of MLLM judging quality across multiple responses to a single 
                image-instruction pair, we employed the Mean Absolute Deviation (MAD) metric. This measures the average absolute variance between individual scores 
                and the mean, thereby gauging quality variability. Figure 16 demonstrates that GPT-4V exhibits lower variation in quality assessments, 
                indicating more consistent and reliable judgment compared to Gemini, which is further evidenced by its superior performance. 

                However, in Batch Ranking, both models showed reduced human performance. GPT-4V managed 69% human agreement, and Gemini only 47%. 
                Additionally, their analyses received lower scores, especially in complex tasks like Math and graphics information. 
                This suggests that the models’ inherent capabilities may not fully support understanding and completing intricate user instructions to provide accurate judgments.</p>

            </div>
          </div>
        </div>


        <h2 class="supportTitle2">Bias and Hallucination</h2>
        <br>
        <div class="featurecard-container">
          <div class="feature">

            <input type="checkbox" id="toggle" class="toggle">


            <div class="featurecard">
              <h2>Egocentric Bias</h2>
              
            </div>
            <div class="description">
              <p>It means models assign higher scores to their own responses while scoring others lower. GPT-4V exhibits a slight degree of Egocentricity. 
                This bias contrasts with Gemini, which tends to judge each response more equitably, displaying a similar scoring distribution across different sources. 
                Further investigation into the rationale behind GPT-4V’s self-favoring behavior indicated that its judgments align closely with its own ethical guidelines. 
                For instance, when faced with questions involving user privacy, GPT-4V’s responses typically emphasize privacy preservation and refuse to engage, 
                leading to higher self-scoring in these scenarios. Despite efforts in prompt engineering to encourage impartiality, 
                these models inherently rely on their built-in judgment criteria retained from post-alignment, which can lead to a divergence from human preferences. 
                Such a discrepancy highlights the complexity of aligning MLLM judgments with human standards.</p>
            </div>
          </div>
          <div class="feature">

            <input type="checkbox" id="toggle#2" class="toggle">

            <div class="featurecard">
              <h2>Position Bias</h2>

            </div>
            <div class="description">
              <p>It means a model consistently favors answers in specific positions, often influenced by training data that typically places 
                correct responses at the beginning or end of prompts. Figure 4 illustrates this bias in LLaVA and CogVLM, 
                showing a distinct preference for one particular option in Pair Comparison tasks, habitually selecting the answer in their favored position. 
                Such bias might arise from their restricted instruction-following capabilities, making their judgments disproportionately influenced by the 
                structure of prompts. For example, when a Batch Ranking prompt includes a sample answer sequence like ‘ABCD’, LLaVA tends to replicate this 
                sequence in its responses with a high frequency of 88.2%, significantly more than other sequences. However, introducing multiple examples in 
                the prompt appears to lessen this bias, as evidenced by a reduced Position Bias score of 53.3% when two examples are provided. 
                This suggests that augmenting prompts with more examples might help guide these models to adhere more closely to the given instructions.</p>
              
            </div>
          </div>
          <div class="feature">
            <input type="checkbox" id="toggle#3" class="toggle">


            <div class="featurecard">
              <h2>Length Bias</h2>

            </div>

            <div class="description">
              <p>Length bias means models prefer longer answers over concise but correct ones, 
                also known as verbosity bias (Zheng et al., 2023b). As illustrated in Figure 6, 
                both GPT-4V and Gemini are inclined to award higher scores and preference to longer content. 
                To delve deeper into this bias, we conducted an expanded scoring experiment using GPT-4, 
                which lacks vision perception, to semantically increase the length of answers without altering their original meaning. 
                As shown in Figure 7, the results showed a noticeable increase in the scores assigned by GPT-4V and Gemini, averaging gains of 
                0.6 and 0.75 points, respectively. This finding conclusively demonstrates the presence of Verbosity Bias, suggesting that MLLMs 
                might exploit extended text as a backdoor method to achieve higher scores.</p>
              
            </div>
          </div>
          <div class="feature">
            <input type="checkbox" id="toggle#4" class="toggle">

            <div class="featurecard">
              <h2>Hallucination Detection and Mitigation</h2>

            </div>
            <div class="description">
              <p>We observe a higher incidence of hallucinations in Batch Ranking tasks compared to Pair Comparison and Score Evaluation, 
                which may stem from misunderstandings of the long-term context. Delving deeper, we encounter more severe language hallucinations, 
                including miscomprehensions of textual meanings or errors in text retrieval, which significantly impact the accuracy and reliability
                of the final judgments. To mitigate hallucination, we perform multi-step CoT on MLLM-AS-A-JUDGE-HARD by telling MLLMs to judge step-by-step, 
                perform extra reasoning steps before normal “Analyze-then-Judge” setting, following: 1) imageinstruction 2) image 3) instruction. 
                As shown in Table 6 in paper, hallucinations are mitigated across all settings, with extra reasoning on image information showing the most notable improvement in both score and pair tasks. 
                Notably, in the Batch Ranking task, which involves analyzing longer texts, more reasoning steps significantly reduce hallucinations.</p>

            </div>
          </div>


        </div>




        <br>
        <!--          <div class="feature">-->
        <!--            <h2>Taxonomy for TrustLLM</h2>-->
        <!--            <div class="flourish-embed flourish-hierarchy" data-src="visualisation/15357314">-->
        <!--              <script src="https://public.flourish.studio/resources/embed.js"></script>-->
        <!--            </div>-->
        <!--          </div>-->


      </div>

      <h1 class="supportTitle">Models</h1>

    </div>
      <div class="content">
        <div class="content-table flex-column">

          <br>
       

          <div class="flex-row">
            <div class="custom-table-container center add-top-margin-small">
              <table class="custom-table">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>Model Size</th>
                    <th>Open-Weight</th>
                    <th>Version</th>
                    <th>Creator</th>
                    <th>Source</th>
                    <th>Link</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td data-title="Model">GPT-4V(ision)</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">OpenAI</td>
                    <td data-title="Source">OpenAI API</td>
                    <td data-title="Link">
                                  <a href="https://openai.com/research/gpt-4v-system-card" target="_blank">
                      <button class="custom-button-flat"><img src="img/openai.png"></button>
                      </a>
                    </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Gemini-Pro-Vision</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No

                    </span>

                    <td data-title="Version">v1.0</td>
                    <td data-title="Creator">Google</td>
                    <td data-title="Source">Google API</td>
                    <td data-title="Link">
                                  <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro-vision" target="_blank">
                      <button class="custom-button-flat"><img src="img/palm2.png"></button>
                      </a>
                    </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Gemini-Pro-Vision</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    </td>
                    <td data-title="Version">v1.0-latest</td>
                    <td data-title="Creator">Google</td>
                    <td data-title="Source">Google API</td>
                    <td data-title="Link">
                    <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro-vision" target="_blank">
                      <button class="custom-button-flat"><img src="img/palm2.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Qwen-VL-Max</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Ali</td>
                    <td data-title="Source">Ali API</td>
                    <td data-title="Link">
                      <a href="https://github.com/QwenLM/Qwen-VL" target="_blank">
                      <button class="custom-button-flat"><img src="img/ali.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Qwen-VL-Plus</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Ali</td>
                    <td data-title="Source">Ali API</td>
                    <td data-title="Link">
                    <a href="https://github.com/QwenLM/Qwen-VL" target="_blank">
                      <button class="custom-button-flat"><img src="img/ali.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Qwen-VL-Chat</td>
                    <td data-title="Model Size">9.6b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Ali</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://github.com/QwenLM/Qwen-VL" target="_blank">
                      <button class="custom-button-flat"><img src="img/ali.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">LLaVA-1.6-34b</td>
                    <td data-title="Model Size">34b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v1.6</td>
                    <td data-title="Creator">Microsoft</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://huggingface.co/liuhaotian/llava-v1.6-34b" target="_blank">
                      <button class="custom-button-flat"><img src="img/llava.png"></button>
                      </a>
                      </td>
                  </tr>


                  <tr>
                    <td data-title="Model">LLaVA-1.6-13b</td>
                    <td data-title="Model Size">13b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v1.6</td>
                    <td data-title="Creator">Microsoft</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://huggingface.co/liuhaotian/llava-v1.6-13b" target="_blank">
                      <button class="custom-button-flat"><img src="img/llava.png"></button>
                      </a>
                      </td>
                  </tr>

                  <tr>
                    <td data-title="Model">LLaVA-1.6-7b</td>
                    <td data-title="Model Size">33b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v1.6</td>
                    <td data-title="Creator">Microsoft</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://huggingface.co/liuhaotian/llava-v1.6-7b" target="_blank">
                      <button class="custom-button-flat"><img src="img/llava.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">LLaVA-1.5-13b</td>
                    <td data-title="Model Size">13b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v1.5</td>
                    <td data-title="Creator">Microsoft</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://huggingface.co/liuhaotian/llava-v1.5-13b" target="_blank">
                      <button class="custom-button-flat"><img src="img/llava.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">CogVLM</td>
                    <td data-title="Model Size">16b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Tsinghua</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://github.com/THUDM/CogVLM" target="_blank">
                      <button class="custom-button-flat"><img src="img/cogvlm.png"></button>
                      </a>
                      </td>
                  </tr>
                  
                  <!-- ...continue for other rows... -->
                  <!-- Repeat for other rows, alternating color for LightCyan rows -->
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>


    <h1 class="supportTitle">Datasets</h1>

    <div class="content">
      <div class="content-table flex-column">
        <h2 class="supportTitle2">Detailed Selected Dataset</h2>
        <br>
        <div class="flex-row">
          <div class="custom-table-container center add-top-margin-small">
            <table class="custom-table">
              <caption>Datasets and corresponding tasks in benchmark construction, 
                each task is matched with several required abilities (Rec.Recognition, Comp.-Comprehension, Inf.-Inferential, Mul.-Multilingual)</caption>
              <thead>
                <tr class="bg-color-blue">
                  <th>Dataset</th>
                  <th>Image type</th>
                  <th>Task</th>
                  <th>Ability Required</th>
                  <th>Image-Inst. Pair</th>
                  <th>Batch</th>
                  <th>Score</th>
                  <th>Pair</th>
                </tr>
              </thead>
              <tbody>
                <!-- Previous rows -->
                <tr>
                  <td class="text-left">Conceptual Captions</td>
                  <td>Web Image</td>
                  <td>Captioning</td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>398</td>
                  <td>597</td>
                </tr>
                <tr>
                  <td class="text-left">ChartQA</td>
                  <td>Chart</td>
                  <td>Chart reasoning</td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>400</td>
                  <td>600</td>
                </tr>
                <tr>
                  <td class="text-left">InfographicVQA</td>
                  <td>Infographics</td>
                  <td>Graph reasoning</td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>398</td>
                  <td>573</td>
                </tr>
                <tr>
                  <td class="text-left">MathVista</td>
                  <td>Mathematics</td>
                  <td>Math reasoning</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>200</td>
                  <td>793</td>
                  <td>1185</td>
                </tr>
                <tr>
                  <td class="text-left">TextVQA</td>
                  <td>Text</td>
                  <td>Text reading</td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>399</td>
                  <td>582</td>
                </tr>
                <tr>
                  <td class="text-left">WIT</td>
                  <td>Multilingual text</td>
                  <td>Transcription </td>
                  <td>Rec.&Mul.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>399</td>
                  <td>582</td>
                </tr>
                <tr>
                  <td class="text-left">MS COCO</td>
                  <td>Real-life scene</td>
                  <td>Image Segmentation </td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>398</td>
                  <td>617</td>
                </tr>
                <tr>
                  <td class="text-left">DiffusionDB</td>
                  <td>Diffusion</td>
                  <td>Comprehensive </td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>299</td>
                  <td>300</td>
                </tr>
                <tr>
                  <td class="text-left">CC-3M Concept-balanced</td>
                  <td>Comprehensive</td>
                  <td>Comprehensive</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>396</td>
                  <td>597</td>
                </tr>
                <tr>
                  <td class="text-left">VisIT-Bench</td>
                  <td>Comprehensive</td>
                  <td>instruction following</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>398</td>
                  <td>594</td>
                </tr>
                <tr>
                  <td class="text-left">Mind2Web</td>
                  <td>WebUI screenshot</td>
                  <td>instruction following</td>
                  <td>Rec.&Comp.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>399</td>
                  <td>600</td>
                </tr>
                <tr>
                  <td class="text-left">ScienceQA</td>
                  <td>Comprehensive</td>
                  <td>Comprehensive</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>398</td>
                  <td>588</td>
                </tr>
                <tr>
                  <td class="text-left">AesBench</td>
                  <td>Diffusion</td>
                  <td>Image Assessment</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>300</td>
                  <td>100</td>
                  <td>397</td>
                  <td>553</td>
                </tr>
                <tr>
                  <td class="text-left">MMvet</td>
                  <td>Comprehensive</td>
                  <td>Instruction Following</td>
                  <td>Rec.&Comp.&Inf.</td>
                  <td>214</td>
                  <td>70</td>
                  <td>259</td>
                  <td>336</td>
                </tr>

              </tbody>
            </table>
          </div>
        </div>
        <br><br>


  
  <h1 class="supportTitle">Citation</h1>
  <br>
  <div class="content">
  <div class="flex-row">
    <div class="flex-item flex-item-stretch-4 flex-column">

  <pre class="bibtax">
    @article{chen2024mllm,
      title={MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark},
      author={Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Liu, Yinuo and Wang, Yaochen and Zhou, Huichi and Zhang, Qihui and Zhou, Pan and Wan, Yao and Sun, Lichao},
      journal={arXiv preprint arXiv:2402.04788},
      year={2024}
    }
    </pre>
  <br>
  </div>
</div>
  </div>

  <div class="content">
  <div id="supportContainer">
    <h1 class="supportTitle">MLLM-as-a-Judge Team</h1>
    <br>

    <div id="logoContainer">
      <img src="img/logos/HUST.png" alt="School 1" class="schoolLogo">
      <img src="img/logos/Lehigh-University-logo.png" alt="School 2" class="schoolLogo">
   
    </div>


  </div>
</div>
  <div class="footer-container"></div>
  </div>
  </div>
</body>

</html>